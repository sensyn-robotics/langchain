{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea857b1",
   "metadata": {},
   "source": [
    "# Build a Local RAG Application\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "\n",
    "- [Chat Models](/docs/concepts/chat_models)\n",
    "- [Chaining runnables](/docs/how_to/sequence/)\n",
    "- [Embeddings](/docs/concepts/embedding_models)\n",
    "- [Vector stores](/docs/concepts/vectorstores)\n",
    "- [Retrieval-augmented generation](/docs/tutorials/rag/)\n",
    "\n",
    ":::\n",
    "\n",
    "The popularity of projects like [llama.cpp](https://github.com/ggerganov/llama.cpp), [Ollama](https://github.com/ollama/ollama), and [llamafile](https://github.com/Mozilla-Ocho/llamafile) underscore the importance of running LLMs locally.\n",
    "\n",
    "LangChain has integrations with [many open-source LLM providers](/docs/how_to/local_llms) that can be run locally.\n",
    "\n",
    "This guide will show how to run `LLaMA 3.1` via one provider, [Ollama](/docs/integrations/providers/ollama/) locally (e.g., on your laptop) using local embeddings and a local LLM. However, you can set up and swap in other local providers, such as [LlamaCPP](/docs/integrations/chat/llamacpp/) if you prefer.\n",
    "\n",
    "**Note:** This guide uses a [chat model](/docs/concepts/chat_models) wrapper that takes care of formatting your input prompt for the specific local model you're using. However, if you are prompting local models directly with a [text-in/text-out LLM](/docs/concepts/text_llms) wrapper, you may need to use a prompt tailed for your specific model. This will often [require the inclusion of special tokens](https://huggingface.co/blog/llama2#how-to-prompt-llama-2). [Here's an example for LLaMA 2](https://smith.langchain.com/hub/rlm/rag-prompt-llama).\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we'll need to set up Ollama.\n",
    "\n",
    "The instructions [on their GitHub repo](https://github.com/ollama/ollama) provide details, which we summarize here:\n",
    "\n",
    "- [Download](https://ollama.com/download) and run their desktop app\n",
    "- From command line, fetch models from [this list of options](https://ollama.com/library). For this guide, you'll need:\n",
    "  - A general purpose model like `llama3.1:8b`, which you can pull with something like `ollama pull llama3.1:8b`\n",
    "  - A [text embedding model](https://ollama.com/search?c=embedding) like `nomic-embed-text`, which you can pull with something like `ollama pull nomic-embed-text`\n",
    "- When the app is running, all models are automatically served on `localhost:11434`\n",
    "- Note that your model choice will depend on your hardware capabilities\n",
    "\n",
    "Next, install packages needed for local embeddings, vector storage, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7914e",
   "metadata": {},
   "source": [
    "You can also [see this page](/docs/integrations/text_embedding/) for a full list of available embeddings models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7543fa",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Now let's load and split an example document.\n",
    "\n",
    "We'll use a [blog post](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng on agents as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8cf5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d5059",
   "metadata": {},
   "source": [
    "Next, the below steps will initialize your vector store. We use [`nomic-embed-text`](https://ollama.com/library/nomic-embed-text), but you can explore other providers or options as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdce8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29137915",
   "metadata": {},
   "source": [
    "And now we have a working vector store! Test that similarity search is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c55e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b43339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf81052",
   "metadata": {},
   "source": [
    "Next, set up a model. We use Ollama with `llama3.1:8b` here, but you can [explore other providers](/docs/how_to/local_llms/) or [model options depending on your hardware setup](https://ollama.com/library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1176bb-d52a-4cf0-b983-8b7433d45b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f7adf",
   "metadata": {},
   "source": [
    "Test it to make sure you've set everything up properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0162e0-8c41-4344-88ae-ff2bbaeb12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The scene is set: A dark, crowded nightclub in Brooklyn. The crowd is rowdy and ready for the ultimate showdown. In the blue corner, we have Stephen Colbert, aka \"The Truth Teller.\" In the red corner, it's John Oliver, aka \"The Last Word.\" The DJ spins a hot track, and the battle begins.**\n",
      "\n",
      "**Stephen Colbert (The Truth Teller)**\n",
      "\n",
      "Yo, Ollie, I heard you've been talkin' smack\n",
      "About me and my show, thinkin' you're whack\n",
      "But let me tell you, boy, I'm the king of late night\n",
      "My satire's sharp, it cuts like a knife in flight\n",
      "\n",
      "You may have your fancy British accent, too\n",
      "But when it comes to truth, I'm the one who breaks through\n",
      "The facade, the lies, the politicians' spin\n",
      "I call 'em out, and the people win\n",
      "\n",
      "**John Oliver (The Last Word)**\n",
      "\n",
      "Hold up, Stevie, let me set you straight\n",
      "Your show's a bit too comfortable, it's not exactly great\n",
      "You're more interested in being liked than right\n",
      "A watered-down version of truth, day and night\n",
      "\n",
      "I may not have the Late Show's ratings game\n",
      "But when I'm done with a topic, it's like a flame\n",
      "That burns away the nonsense, exposes the lie\n",
      "My reporting's thorough, you can't deny\n",
      "\n",
      "**Stephen Colbert (The Truth Teller)**\n",
      "\n",
      "Deny? Ha! You're just jealous of my fame\n",
      "My audience is huge, they love me for what I claim\n",
      "To be honest and funny, a perfect blend\n",
      "You may have your HBO deal, but that's where it ends\n",
      "\n",
      "I'm the one who gets people talking, boy\n",
      "My humor's sharp, it cuts through the noise, oh joy!\n",
      "You're just a British upstart, trying to make a name\n",
      "But when it comes to comedy and truth, I'm still in the game\n",
      "\n",
      "**John Oliver (The Last Word)**\n",
      "\n",
      "Name? Ha! You think you're the one they call?\n",
      "The master of satire, the king of them all?\n",
      "Please, Stevie, your show's been on the air for years\n",
      "And what have you accomplished? Just tears and cheers?\n",
      "\n",
      "I've taken down politicians, industries too\n",
      "My reporting's fearless, I'm not afraid to break through\n",
      "You may have your Colbert Report days in the past\n",
      "But when it comes to substance, I'm the one who lasts\n",
      "\n",
      "**The crowd goes wild as both opponents exchange witty barbs and clever put-downs. In the end, it's a close call, but...**\n",
      "\n",
      "**Stephen Colbert (The Truth Teller)**\n",
      "\n",
      "You know what, Ollie? It's time to concede\n",
      "I think we both brought our A-game to this read\n",
      "But in the end, I'm still the king of late night\n",
      "And you're just the guy who likes to fight.\n",
      "\n",
      "( The crowd cheers as Stephen takes a triumphant bow.)\n",
      "\n",
      "**John Oliver (The Last Word)**\n",
      "\n",
      "Oh no, Stevie, don't get ahead of yourself\n",
      "I may have lost this battle, but the war is what I'll sell\n",
      "Truth and comedy will always be my game\n",
      "And next time, you can bet I'll come back with more claim!\n",
      "\n",
      "( The crowd cheers as John exits the stage with a sly smile.)\n"
     ]
    }
   ],
   "source": [
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58838ae",
   "metadata": {},
   "source": [
    "## Using in a chain\n",
    "\n",
    "We can create a summarization chain with either model by passing in retrieved docs and a simple prompt.\n",
    "\n",
    "It formats the prompt template using the input key values provided and passes the formatted string to the specified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a3716d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these documents appear to be:\\n\\n1. **Task decomposition**: Breaking down complex tasks into smaller, manageable subgoals or steps.\\n2. **Methods for task decomposition**:\\n\\t* Using Large Language Models (LLMs) with simple prompts, such as \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\\n\\t* Employing task-specific instructions, like \"Write a story outline\" for writing a novel\\n\\t* Utilizing human inputs in the task decomposition process\\n3. **Autonomous agent systems**: Overview of a system that leverages LLMs to facilitate planning and decision-making.\\n\\nThese themes seem to be related to artificial intelligence, machine learning, and problem-solving strategies.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce6977-52e7-4944-89b4-c161d04f6698",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "\n",
    "You can also perform question-answering with your local model and vector store. Here's an example with a simple string prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67cefb46-acd3-4c2a-a8f6-b62c7c3e30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to Task Decomposition: (1) using Language Models (LLM) with simple prompting, (2) utilizing task-specific instructions, and (3) incorporating human inputs. These methods help break down complex tasks into manageable subgoals. This process aids in planning and decision-making for autonomous agents.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821729cb",
   "metadata": {},
   "source": [
    "## Q&A with retrieval\n",
    "\n",
    "Finally, instead of manually passing in docs, you can automatically retrieve them from our vector store based on the user question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c7a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "112ca227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to Task Decomposition: using Large Language Models (LLM) with simple prompting, using task-specific instructions, and with human inputs. This can be done by LLMs through prompting like \"Steps for XYZ.\" or \"Write a story outline.\", or by directly asking humans for input.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d3e9e",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now seen how to build a RAG application using all local components. RAG is a very deep topic, and you might be interested in the following guides that discuss and demonstrate additional techniques:\n",
    "\n",
    "- [Video: Reliable, fully local RAG agents with LLaMA 3](https://www.youtube.com/watch?v=-ROS6gfYIts) for an agentic approach to RAG with local models\n",
    "- [Video: Building Corrective RAG from scratch with open-source, local LLMs](https://www.youtube.com/watch?v=E2shqsYwxck)\n",
    "- [Conceptual guide on retrieval](/docs/concepts/retrieval) for an overview of various retrieval techniques you can apply to improve performance\n",
    "- [How to guides on RAG](/docs/how_to/#qa-with-rag) for a deeper dive into different specifics around of RAG\n",
    "- [How to run models locally](/docs/how_to/local_llms/) for different approaches to setting up different providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-monorepo-py3.12",
   "language": "python",
   "name": "langchain-monorepo-py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
